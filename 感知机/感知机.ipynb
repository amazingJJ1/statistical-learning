{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "1．感知机是根据输入实例的特征向量$x$对其进行二类分类的线性分类模型：\n",
    "\n",
    "$$\n",
    "f(x)=\\operatorname{sign}(w \\cdot x+b)\n",
    "$$\n",
    "\n",
    "感知机模型对应于输入空间（特征空间）中的分离超平面$w \\cdot x+b=0$。\n",
    "\n",
    "2．感知机学习的策略是极小化损失函数：\n",
    "\n",
    "$$\n",
    "\\min _{w, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "损失函数对应于误分类点到分离超平面的总距离。\n",
    "\n",
    "3．感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式。算法简单且易于实现。原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。在这个过程中一次随机选取一个误分类点使其梯度下降。\n",
    " \n",
    "4．当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数$k$满足不等式：\n",
    "\n",
    "$$\n",
    "k \\leqslant\\left(\\frac{R}{\\gamma}\\right)^{2}\n",
    "$$\n",
    "\n",
    "当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 二分类模型\n",
    "$f(x) = sign(w\\cdot x + b)$\n",
    "\n",
    "$\\operatorname{sign}(x)=\\left\\{\\begin{array}{ll}{+1,} & {x \\geqslant 0} \\\\ {-1,} & {x<0}\\end{array}\\right.$\n",
    "\n",
    "给定训练集：\n",
    "\n",
    "$T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$\n",
    "\n",
    "定义感知机的损失函数 \n",
    "\n",
    "$L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)$\n",
    "\n",
    "---\n",
    "#### 算法\n",
    "\n",
    "随即梯度下降法 Stochastic Gradient Descent\n",
    "\n",
    "随机抽取一个误分类点使其梯度下降。\n",
    "\n",
    "$w = w + \\eta y_{i}x_{i}$\n",
    "\n",
    "$b = b + \\eta y_{i}$\n",
    "\n",
    "当实例点被误分类，即位于分离超平面的错误侧，则调整$w$, $b$的值，使分离超平面向该无分类点的一侧移动，直至误分类点被正确分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}